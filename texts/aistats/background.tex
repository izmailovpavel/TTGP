\subsection{Gaussian Processes}
  A Gaussian process is a collection of random variables, any finite number of
  which have a joint normal distribution. A GP $f$ taking place in $\R^D$ is
  fully defined by its mean $m: \R^D \rightarrow \R$ and covariance
  $k: \R^D \times \R^D \rightarrow \R$ functions. For every $x_1, x_2, \ldots, x_n \in \R^D$
  \[
    f(x_1), f(x_2), \ldots, f(x_n) \sim \N(m, K),
  \]
  where $m = (m(x_1), m(x_2), \ldots, m(x_n))^T \in \R^n$, and
  $K\in \R^{n \times n}$ is the covariance matrix with $K_{ij}=k(x_i,x_j)$. Below we will use notation $K(A, B)$ for the matrix of pairwise
  values of covariance function $k$ on points from sets $A$ and $B$.

  Consider a regression problem. The dataset consists of
  $n$ objects $X = (x_1, \ldots, x_n)^T \in \R^{n \times D}$, and target values
  $y = (y_1, y_2, \ldots, y_n)^T \in \R^n$. We assume that the data is
  generated by a latent zero-mean Gaussian process $f$ plus independent Gaussian noise:
  \begin{equation}
    \begin{split}
      &p(y,f | X) = p(f | X)\prod_{i=1}^np(y_i | f_i),\\
      &p(f | X) = \N(f | 0, K(X,X)), \\
      &p(y_i | f_i) = \N(y_i | f_i, \nu^2 I),\\
    \end{split}
    \label{gp_model}
  \end{equation}
  where $f_i=f(x_i)$ is the value of the process at data point $x_i$ and $\nu^2$ is the noise variance.

  Assume that we want to predict the values of the process $f_*$ at a set of test
  points $X_*$. As the joint distribution of $y$ and $f_*$ is Gaussian, we can analytically
  compute the conditional distribution $p(f_* | y, X, X_*) = \N(f_*|\hat{m},\hat{K})$ 
  with tractable formulas for $\hat{m}$ and $\hat{K}$. The complexity of computing
  $\hat m$ and $\hat K$ is $\bigO(n^3)$ since it involves calculation of the 
  inverse of the covariance matrix $K(X, X)$.
  
  Covariance functions usually have a set of hyper-parameters $\theta$. For example, the RBF kernel
  \[
    k_{\mbox{\scriptsize RBF}} (x, x') = \sigma_f^2 \exp\left(- 0.5 \Vert x - x'\Vert^2 / l^2 \right)
  \]
  has two hyper-parameters $l$ and $\sigma_f$. In order to fit the model to the data,
  we can maximize the marginal likelihood of the process $p(y|X)$ with respect to these
  parameters. In case of GP regression this marginal likelihood is tractable and
  can be computed in $\bigO(n^3)$ operations.

  For two-class classification problem we use the same model~\eqref{gp_model} 
  with $p(y_i | f_i)=1/(1+\exp(-y_if_i))$, where $y_i\in\{-1,+1\}$. In this case
  both predictive distribution and marginal likelihood are intractable. For 
  detailed description of GP regression and classification see \citet{rasmussen2006}.

\subsection{Inducing Inputs}
\label{inducing_inputs}

  A number of approximate methods were developed to scale up Gaussian processes.
  \citet{hensman2013} proposed a variational lower bound that factorizes over
  observations for Gaussian process marginal likelihood. We rederive
  this bound here.

  Consider a set $Z \in \R^{m \times D}$ of $m$ inducing points in the feature 
  space and latent variables $u \in \R^m$ representing the values of
  the Gaussian process at these points. Consider the augmented model
  \[
    p(y, f, u) = p(y | f) p(f | u) p(u) = \prod_{i = 1}^ n p(y_i | f_i) p(f | u) p(u)
  \]
  with 
  \begin{equation}
  \begin{split}
    &p(f | u) = \N(f | \Knm \Kmm^{-1} u, \Knn - \Knm \Kmm^{-1} \Kmn),\label{priors}\\
    &p(u)= \N(u | 0, \Kmm),\notag
  \end{split}
  \end{equation}
  where $\Knn = K(X, X), \Knm = K(X, Z), \Kmn = K(Z, X) = \Knm^T, \Kmm = K(Z, Z)$.

  The standard variational lower bound is given by
  \begin{equation}
  \label{standard_elbo}
  \begin{split}
    &\log p(y) \ge \E_{q(u, f)} \log \frac {p(y, f, u)} {q(u, f)} =\\
    & = \E_{q(f)} \log \prod_{i=1}^n p(y_i | f_i) - \KL(q(u, f) || p(u, f)),
  \end{split}
  \end{equation}
  where $q(u, f)$ is the variational distribution over latent variables.
  Consider the following family of variational distributions
  \begin{equation}
  \label{var_family}
    q(u, f) = p(f | u) \N(u | \mu, \Sigma),
  \end{equation}
  where $\mu \in \R^m$ and $\Sigma \in \R^{m \times m}$ are variational
  parameters. Then the marginal distribution over $f$ can be computed analytically
  \begin{equation}
  \begin{split}
    q(f)  = &\N\left(f | \Knm \Kmm^{-1} \mu,\right.\\
    &\left.\Knn + \Knm \Kmm^{-1} (\Sigma - \Kmm) \Kmm^{-1} \Kmn\right).
  \end{split}
  \end{equation}
  We can then rewrite (\ref{standard_elbo}) as
  \begin{equation}
  \label{main_elbo}
  \begin{split}
    \log p(y) \ge \sum_{i=1}^n & \E_{q(f_i)} \log p(y_i | f_i) - \\
      & \KL(q(u) || p(u)).
  \end{split}
  \end{equation}
  Note, that the lower bound (\ref{main_elbo}) factorizes over observations and
  thus stochastic optimization can be applied to maximize this bound with respect
  to both kernel hyper-parameters $\theta$ and variational parameters $\mu$ and
  $\Sigma$. In case of regression we can rewrite (\ref{main_elbo}) in the closed
  form
  \begin{equation}
    \label{elbo_reg}
    \begin{split}
      \log & p(y) \ge 
      \sum_{i=1}^n  \bigg (\log \N (y_i | \ki^T \Kmm^{-1} \mu, \nu^2) - \\
        &- \frac 1 {2\nu^2} \tilde K_{ii} - 
        \left . \frac 1 {2\nu^2} \tr (\ki^T \Kmm^{-1} \Sigma \Kmm^{-1} \ki)
      \right )-
      \\
      - \frac 1 2 & \bigg(
      \log \frac {|\Kmm|} {|\Sigma|}  - m + \tr(\Kmm^{-1} \Sigma) +
        \mu^T \Kmm^{-1} \mu 
      \bigg),
    \end{split}
  \end{equation}
  where $\ki \in \R^m$ is the $i$-th column of $\Kmn$ matrix and $\tilde{K} = \Knn - \Knm \Kmm^{-1} \Kmn.$
  
  At prediction time we can use the variational distribution as a substitute for
  posterior
  \[
    p(f_* | y) = \int p(f_*| f, u) p(f, u |y) d f d u \approx
  \]
  \[
    \approx \int p(f_* | f, u) q(f, u) d f d u = \int p(f_* | u) q(u) du.
  \]

  The complexity of computing the bound (\ref{elbo_reg}) is $\bigO(n m^2 + m^3)$.
  \citet{hensman2015} proposes to use Gauss-Hermite quadratures to approximate the
  expectation term in (\ref{main_elbo}) for binary classification problem to
  obtain the same computational complexity $\bigO(nm^2 + m^3)$. This complexity
  allows to use Gaussian processes in tasks with millions of training samples,
  but these methods are limited to use small numbers of inducing points $m$,
  which hurts the predictive performance and doesn't allow to learn expressive
  kernel functions.

\subsection{KISS-GP}
\label{kiss_gp}

  \citet{saatci2012} noted that the covariance matrices computed at points on a
  multidimensional grid in the feature space can be represented as a Kronecker
  product if the kernel function factorizes over dimensions
  \begin{equation}
  \label{prod_kernel}
    k(x, x') = k_1(x^1, x'^1)\cdot k_2(x^2, x'^2)\cdot \ldots\cdot k_D(x^D, x'^D).
  \end{equation}
  Note, that many popular covariance functions, including RBF, belong to this class.
  Kronecker structure of covariance matrices allows to perform efficient inference
  for full Gaussian processes with inputs $X$ on a grid.

  \citet{wilson2015} proposed to set inducing inputs $Z$ on a grid:
  \[
    Z = Z^1 \times Z^2 \times \ldots \times Z^D,~~~~~Z^i \in \R^{m_i}~~~\forall i = 1, 2, \ldots, D.
  \]
  The number $m$ of inducing points is then given by $m = \prod_{i=1}^D m_{i}$.

  Let the covariance function satisfy (\ref{prod_kernel}). Then the covariance
  matrix $\Kmm$ can be represented as a Kronecker product over dimensions
  \[
    \Kmm = K_{m_1 m_1}^1 \otimes K^2_{m_2 m_2} \otimes \ldots \otimes
    K^D_{m_D m_D},
  \]
  where
  \[
    K^i_{m_i m_i} = K_i(Z_i, Z_i) \in \R^{m_i \times m_i}~~~\forall i = 1, 2, \ldots, D.
  \]
  Kronecker products allow efficient computation of matrix inverse and determinant:
  \begin{align*}
    &(A_1 \otimes A_2 \otimes \ldots \otimes A_D)^{-1} = A_1^{-1} \otimes A_2^{-1} \otimes \ldots \otimes A_D^{-1},\\
    &|A_1 \otimes A_2 \otimes \ldots \otimes A_D| = |A_1|^{c_1} \cdot |A_2|^{c_2} \cdot \ldots \cdot |A_D|^{c_D},
  \end{align*}
  where $A_i\in\R^{k_i \times k_i}$, $c_i = \prod_{j \ne i} k_j,\forall i = 1, 2, \ldots, D$.
  
  Another major idea of KISS-GP is to use interpolation to approximate $\Kmn$.
  Considering inducing inputs as interpolation points for the function
  $k(\cdot, z_i)$ we can write
  \begin{equation}
  \label{kernel_interpolation}
    \Kmn \approx \Kmm W,~~~\ki \approx \Kmm w_i,
  \end{equation}
  where $W \in \R^{m \times n}$ contains the coefficients of interpolation, and
  $w_i$ is it's $i$-th column. Authors of KISS-GP suggest using cubic
  convolutional interpolation (\citet{keys1981}), in which case the interpolation
  weights $w_i$ can be represented as a Kronecker product over dimensions
  \[
    w_i = w_i^1 \otimes w_i^2 \otimes \ldots \otimes w_i^D,~~~~~w_i \in \R^{m_i}~~~\forall i = 1, 2, \ldots, D.
  \]
  \citet{wilson2015} combine these ideas with SOR (\citet{silverman1985})
  in the KISS-GP method yielding $\bigO(n + D m^{1 + 1/D})$ computational
  complexity. This complexity allows to use KISS-GP with a large number (possibly
  greater than $n$) of inducing points. Note, however, that $m$ grows
  exponentially with the dimensionality $D$ of the feature space and the
  method becomes impractical when $D > 4$.

\subsection{Tensor Train Decomposition}
\label{tensor_train}

  Tensor Train (TT) decomposition, proposed in \citet{oseledets2011}, allows to
  efficiently store tensors (multidimensional arrays of data), large matrices, and
  vectors. For matrices and vectors in the TT-format linear algebra operations
  can be implemented efficiently. The TT format was successfully applied for
  different machine learning tasks (see \citet{novikov2014}, \citet{novikov2015}).

  Consider a $D$-dimensional tensor $\mathcal A \in \R^{k_1 \times k_2 \times \ldots \times k_D}$.
  $\mathcal{A}$ is said to be in the Tensor Train format if
  \begin{equation}
  \label{tt}
    \mathcal{A}(i_1, i_2, \ldots, i_d) = G_1[i_1] \cdot G_2[i_2] \cdot \ldots \cdot G_D[i_D],~~~
    i_k \in \{1, 2, \ldots, n_k\}~~\forall k,
  \end{equation}
  where $G_k[i_k] \in \R^{r_{k-1}\times r_{k}}\ \forall k, i_k,\ r_0 = r_{D} = 1$. Matrices $G_k$ are called TT-cores, and numbers $r_k$ are called TT-ranks of
  tensor $\mathcal{A}$.

  In order to represent a vector in TT-format, it is reshaped to a multidimensional
  tensor (possibly with zero padding) and then format (\ref{tt}) is used. We will
  use TT-format for the vector $\mu$ of expectations of the values $u$ of the
  Gaussian process in points $Z$ placed on a multidimensional grid. In this case,
  $\mu$ is naturally represented as a $D$-dimensional tensor.

  For matrices TT format is given by
  \[
    M(i_1, i_2, \ldots, i_d; j_1, j_2, \ldots, j_D) = G_1 [i_1, j_1] \cdot
    G_2[i_2, j_2] \cdot \ldots \cdot G_D[i_D, j_D],
  \]
  where $G_k[i_k, j_k] \in \R^{r_{k-1}\times r_{k}}\ \forall k, i_k, j_k,\ r_0 = r_{D} = 1$.
  Note, that Kronecker product format is a special case of the TT-matrix with TT-ranks
  $r_1 = r_2 = \ldots = r_{D} = 1$.

  Let $u, v \in \R^{n_1 \cdot n_2 \cdot \ldots \cdot n_D}$ be vectors
  in TT-format with TT-ranks not greater than $r$. Let $A$ and $B$ be represented as a Kronecker product
  \[
    A = A_1 \otimes A_2 \otimes \ldots \otimes A_D,~~~A_k \in \R^{n_k \times n_k}~~\forall k,
  \]
  and the same for $B$. Let $n = \max_k n_k$. Then the computational complexity
  of computing the quadratic form $u^T A v$ and $\tr(AB)$ is $\bigO(Dnr^3)$ and $\bigO(Dn^2)$ correspondingly. We will need these two
  operations below.
