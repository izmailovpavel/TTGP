\documentclass[twoside]{article}
\usepackage[accepted]{aistats2018}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\PassOptionsToPackage{numbers}{natbib}
%\usepackage[final]{nips_2017}
%\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[round]{natbib}

\newcommand{\bigO}{\mathcal{O}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Knn}{K_{nn}}
\newcommand{\Knm}{K_{nm}}
\newcommand{\Kmn}{K_{mn}}
\newcommand{\Kmm}{K_{mm}}
\newcommand{\ki}{k_{i}}
\newcommand{\KL}{\mbox{KL}}
\newcommand{\tr}{\mbox{tr}}
%\renewcommand{\baselinestretch}{0.95}
%\parskip .43pc  % original size - .5pc

\begin{document}
\twocolumn[

\aistatstitle{Scalable Gaussian Processes with Billions of Inducing Inputs via 
  Tensor Train Decomposition}

\aistatsauthor{ Pavel A.~Izmailov \And Alexander V.~Novikov \And  
Dmitry A.~Kropotov }

\aistatsaddress{ 
  Lomonosov Moscow State\\ University, \\
  Cornell University\And
  National Research University\\ Higher School of Economics,\\
  Institute of Numerical Mathematics RAS\And
  Lomonosov Moscow State\\ University
}
]


%\aistatsaddress{ Institution 1 \And  Institution 2 \And Institution 3 } ]

%\author{
%  Pavel A.~Izmailov \\
%  Lomonosov Moscow State University\\
%  Cornell University\\
%  \texttt{izmailovpavel@gmail.com} \\
%  %% examples of more authors
%  \And
%  Alexander V.~Novikov\\
%  National Research University Higher School of Economics \\
%  \texttt{novikov@bayesgroup.ru} \\
%  \AND
%  Dmitry A.~Kropotov \\
%  Lomonosov Moscow State University\\
%  \texttt{dmitry.kropotov@gmail.com} \\
%}

\begin{abstract}
  We propose a method (TT-GP) for approximate inference in Gaussian Process (GP) 
  models. We build on previous scalable GP research including stochastic 
  variational inference based on inducing inputs, kernel interpolation, and 
  structure exploiting algebra.
  The key idea of our method is to use
  Tensor Train decomposition for variational parameters, which allows us to train
  GPs with billions of inducing inputs and achieve state-of-the-art results
  on several benchmarks. Further, our approach allows for training kernels based on
  deep neural networks without any modifications to the underlying GP model.
  A neural network learns a multidimensional embedding for the data, which is
  used by the GP to make the final prediction.
  %We train GP and neural network parameters end-to-end
  without pretraining, through maximization of GP marginal likelihood.
  We show the efficiency of the
  proposed approach on several regression and classification benchmark datasets
  including MNIST, CIFAR-10, and Airline.

\end{abstract}

\section{Introduction}
  \input{introduction}

\section {Background}
  \input{background}

\section{TT-GP}
  \input{ttgp}

\section{Experiments}
  \input{experiments}

\section{Discussion}
  \input{discussion}

\subsubsection*{Acknowledgements}

Alexander Novikov was supported by the Russian Science Foundation grant 17-11-01027.
Dmitry Kropotov was supported by Samsung Research, Samsung Electronics.

%\nocite{*}
\bibliography{bib/biblio}
\bibliographystyle{plainnat}

\end{document}
