In the previous section we described several methods for GP regression and
classification. All these methods have different limitations: standard methods
are limited to small-scale datasets, KISS-GP requires small dimensionality of
the feature space, and other methods based on inducing inputs are limited to use
a small number $m$ of these points. In this section, we propose the TT-GP method
that can be used with big datasets and can incorporate billions of inducing
inputs. Additionally, TT-GP allows for training expressive deep kernels to work
with structured data (e.g. images).

\subsection{Variational Parameters Approximation}
  In section \ref{inducing_inputs} we derived the variational lower bound of
  \citet{hensman2013}. We will place the inducing inputs $Z$ on a
  multidimensional grid in the feature space and we will assume the
  covariance function satisfies (\ref{prod_kernel}). Let the number
  of inducing inputs in each dimension be $m_0$. Then the total number of inducing 
  inputs is $m = m_0^D$.
  As shown in Section \ref{kiss_gp},
  in this case $\Kmm$ matrix can be expressed as a Kronecker product over
  dimensions. Substituting the approximation (\ref{kernel_interpolation}) into
  the lower bound (\ref{elbo_reg}), we obtain
  \begin{equation}
  \label{kissgp_elbo}
    \begin{split}
      &\log p(y) \ge
        \\
        &\sum_{i=1}^n \bigg ( \log \N (y_i | w_i^T \mu, \nu^2) -
        \frac 1 {2\nu^2} \tilde K_{ii} - 
        \frac 1 {2\nu^2} \tr (w_i^T \Sigma w_i)
      \bigg )-
      \\
      &- \frac 1 2 \left(
        \log \frac {|\Kmm|} {|\Sigma|} - m + \tr(\Kmm^{-1} \Sigma) +
        \mu^T \Kmm^{-1} \mu
      \right),
    \end{split}
  \end{equation}
  where $\tilde K_{ii} = k(x_i, x_i) - w_i^T \Kmm w_i$.

  Note that $\Kmm^{-1}$ and $|\Kmm|$ can be computed with
  $\bigO(D m_0^3) = \bigO(D m^{3/D})$ operations due to the
  Kronecker product structure. Now the most computationally demanding terms
  are those containing variational parameters $\mu$ and $\Sigma$.

  Let us restrict the family of variational distributions~(\ref{var_family}). Let $\Sigma$ be
  a Kronecker product over dimensions, and $\mu$ be in the TT-format whith 
  TT-rank $r$ ($r$ is a hyper-parameter of our method). Then, according to 
  section \ref{tensor_train}, we can compute the lower bound~\eqref{kissgp_elbo}
  with
  $\bigO(nDm_0 r^2 + D m_0 r^3 + Dm_0^3) =
  \bigO(nD m^{1/D} r^2 + D m^{1/D} r^3 + D m^{3/D})$ complexity.

  The proposed TT-GP method has linear complexity with respect to dimensionality
  $D$ of the feature space, despite the exponential growth of the number of
  inducing inputs. Lower bound  (\ref{kissgp_elbo})
  can be maximized with respect to kernel hyper-parameters~$\theta$, TT-cores
  of $\mu$, and Kronecker multipliers of $\Sigma$. Note that stochastic optimization
  can be applied, as the bound (\ref{kissgp_elbo}) factorizes over data points.
  
  TT format was successfully applied for different machine learning tasks, 
  e.g. for compressing neural networks (\citet{novikov2015}) and
  estimating log-partition function in probabilistic graphical models 
  (\citet{novikov2014}). We explore the properties of approximating the 
  variational mean $\mu$ in TT format in section \ref{expect_approx}.



  \subsection{Classification}

  In this section we describe a generalization of the proposed method for
  multiclass classification. In this case the dataset consists of features
  $X = (x_1, x_2, \ldots, x_n)^T \in \R^{n \times D}$ and target values
  $y = (y_1, y_2, \ldots, y_n)^T \in \{1, 2, \ldots, C\}^n$, where $C$ is the
  number of classes.

  Consider $C$ Gaussian processes taking place in $\R^D$. Each process
  corresponds to it's own class. We will place $m = m_0^D$ inducing inputs $Z$ on a grid
  in the feature space, and they will be shared between all processes. Each
  process has it's own set of latent variables representing the values of
  the process at data points $f^c \in \R^n$, and inducing inputs $u^c \in \R^m$.
  We will use the following model
  \[
    p(y, f, u) = \prod_{i=1}^n p\left(y_i | f_i^{1, 2, \ldots, C}\right)
      \prod_{c=1}^C p\left(f^c | u^c\right) p(u^c),
  \]
  where $f_i^{1,2,\ldots,C}$ is the vector consisting of the values of all processes
  $1, 2, \ldots, C$ at data point $i$, $p(f^c | u^c)$ and $p(u^c)$ are defined 
  as in (\ref{priors}) and 
  \[
    p(y_i | f_i^{1,2, \ldots, C}) = 
    \frac {\exp(f_i^{y_i})} {\sum_{j=1}^C\exp(f_i^j)}.
  \]
  
  We will use variational distributions of the form
  \[
    q(f^1, f^2, \ldots, f^C, u^1, u^2, \ldots, u^C) =
  \]
  \[
    = q(f^1, u^1) \cdot q(f^2, u^2) \cdot \ldots \cdot q(f^C, u^C),
  \]
  where $q(f^c, u^c) = p(f^c | u^c) \N(u^c | \mu^c, \Sigma^c), c~=~1,~2,~\ldots,\\C$,
  all $\mu^c$ are represented in TT-format with TT-ranks not greater than $r$
  and all  $\Sigma^c$ are represented as Kronecker products over dimensions. 
  Similarly to (\ref{main_elbo}), we obtain
  \begin{equation}
  \label{elbo_multiclass}
  \begin{split}
    \log p(y) \ge & \sum_{i = 1}^n \E_{q(f_i^{1, 2, \ldots, C})} \log p(y_i | f_i^{1, 2, \ldots, C})
    \\
    & - \sum_{c = 1}^{C} \KL(q(u_c) || p(u_c))
  \end{split}
  \end{equation}
  The second term in (\ref{elbo_multiclass}) can be computed analytically as
  a sum of KL-divergences between normal distributions. The first term is
  intractable. In order to approximate the first term we will use a lower bound.
  Let $y_i$ belong to class $c$. Then, we can rewrite
  \begin{equation}
  \label{expectation_term}
  \begin{split}
    &\E_{q\left(f_i^{1, 2, \ldots, C}\right)} \log p(y_i | f_i^{1, 2, \ldots, C}) =\\
    &\E_{q(f_i^c)} f_i^c - \E_{q\left(f_i^{1, 2, \ldots, C}\right)}
    \log \biggl(\sum_{j=1}^C \exp (f_i^j) \biggr),
  \end{split}
  \end{equation}
  where $q(f_i^{1, 2\ldots, C}) = \N(f^1 | m_i^1, s_i^1)\cdot \N(f^2| m_2, s_i^2) \cdot \ldots \cdot \N(f^C | m_i^C, s_i^C)$.

  The first term in (\ref{expectation_term}) is obviously tractable, while the
  second term has to be approximated. \citet{bouchard2007} discusses several
  lower bounds for expectations of this type. Below we derive one of these bounds,
  which we use in TT-GP.

  Concavity of logarithm implies $\log\left(\sum_{j=1}^{C} \exp(f_i^j)\right) \le 
  \varphi \sum_{j=1}^C \exp(f_i^j) - \log \varphi - 1,\ \forall\varphi > 0$. 
  Taking expectation of both sides of the inequality and minimizing with respect
  to $\varphi$, we obtain
  \begin{equation}
  \label{bouchard_bound}
  \begin{split}
    \E_{q(f_i^{1, 2, \ldots, C})} \log\biggl(\sum_{j=1}^C \exp(f_i^j)\biggr) \le
    \\
    \log \biggl(\sum_{j=1}^C \exp\bigl(m_i^j + \frac 1 2 s_i^j\bigr) \biggr).
  \end{split}
  \end{equation}
  Substituting (\ref{bouchard_bound}) back into (\ref{elbo_multiclass}) we obtain
  a tractable lower bound for multiclass classification task, that can be
  maximized with respect to kernel hyper-parameters~$\theta^c$, TT-cores of
  $\mu^c$ and Kronecker factors of $\Sigma^c$. The complexity of the method
  is $C$ times higher, than in regression case.

\subsection{Deep kernels}

  \citet{wilson2016stochastic} and \citet{wilson2016deep} showed the efficiency
  of using expressive kernel functions based on deep neural networks with
  Gaussian processes on a variety of tasks. The proposed TT-GP method is
  naturally compatible with this idea.

  Consider a covariance function $k$ satisfying (\ref{prod_kernel}) and
  a neural network (or in fact any parametric transform) $net$. We can define a
  new kernel as follows
  \[
    k_{net}(x, x') = k(net(x), net(x')).
  \]
  We can train the neural network weights through maximization of GP marginal
  likelihood, the same way, as we normally train kernel hyper-parameters $\theta$.
  This way, the network learns a multidimensional embedding for the data, and
  GP is making the prediction working with this embedding.
  
  \citet{wilson2016stochastic} trained additive deep kernels combining 
  one-dimensional GPs on different outputs of a neural network. Training
  Gaussian processes on multiple outputs of a Neural network is impractical
  in their framework, because the complexity of the GP part of their model
  grows exponentially with the input dimensionality. 

  With methods of \citet{hensman2013} and \citet{hensman2015} training Gaussian
  processes on multiple outputs of a neural network also isn't straightforward. Indeed,
  with these methods we can only use up to $10^3$--$10^4$ inducing inputs. While
  with standard RBF kernels the positions of inputs of the GP are fixed and we can place 
  the inducing inputs near the data, with deep kernels the positions of the
  inputs of the GP (outputs of the DNN) change during training to match the positions of inducing inputs.
  It is thus not clear how to set the inducing inputs in the latent feature space, other
  than placing them on a multidimensional grid, which means that the complexity
  of such methods would grow exponentially with dimensionality.

  On the other hand, TT-GP allows us to train Gaussian processes on multiple DNN
  outputs because of it's ability to efficiently work with inducing inputs 
  placed on multidimensional grids.
